# NexusDrive-ETL-pipeline-Airflow

ETL pipeline for fetching Weather, Traffic, and Delivery data, and loading it into PostgreSQL.

## Overview

This project implements an ETL (Extract, Transform, Load) pipeline using Apache Airflow. The pipeline processes delivery data, weather data, and traffic data, enriches them, and aligns them with Amazon delivery data. The final output is stored in a PostgreSQL database.

## Features

- **Data Extraction**: Fetches delivery and weather data from MinIO (S3-compatible storage).
- **Data Transformation**: Enriches delivery data with weather, traffic, and vehicle data.
- **Data Alignment**: Aligns enriched data with Amazon delivery data.
- **Data Loading**: Stores the final aligned dataset in PostgreSQL.

## Prerequisites

- Docker and Docker Compose
- Python 3.8+
- MinIO (S3-compatible storage)
- PostgreSQL

## Setup Instructions

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-repo/NexusDrive-ETL-pipeline-Airflow.git
   cd NexusDrive-ETL-pipeline-Airflow
   ```

2. **Configure Environment Variables**:
   Create a `.env` file in the project root with the following variables:
   ```
   AIRFLOW_UID=50000
   AIRFLOW_PROJ_DIR=.
   ENV_FILE_PATH=.env
   ```

3. **Build and Start the Services**:
   ```bash
   docker-compose up --build
   ```

4. **Access Airflow**:
   Open your browser and navigate to `http://localhost:8080`. Use the default credentials:
   - Username: `airflow`
   - Password: `airflow`

5. **Upload Data to MinIO**:
   - Ensure that delivery and weather data files are uploaded to the appropriate MinIO buckets.

6. **Trigger the DAG**:
   - In the Airflow UI, trigger the `etl_delivery_pipeline` DAG.

## Project Structure

- `dags/`: Contains the Airflow DAGs.
- `docker-compose.yaml`: Docker Compose configuration for the Airflow cluster.
- `plugins/`: Custom plugins for Airflow.
- `config/`: Configuration files for Airflow.
- `logs/`: Logs generated by Airflow.

## Key Components

- **MinIO**: Used as the storage backend for delivery and weather data.
- **PostgreSQL**: Stores the final aligned dataset.
- **Airflow**: Orchestrates the ETL pipeline.

## Usage

1. Upload the required data files to MinIO:
   - Delivery data: `Pickup_and_delivery_data/delivery/delivery_<city>.csv`
   - Weather data: `Pickup_and_delivery_data/weather/<city>_weather.csv`
   - Amazon delivery data: `Pickup_and_delivery_data/delivery/amazon_delivery.csv`

2. Trigger the DAG in the Airflow UI.

3. Monitor the DAG execution and verify the output in PostgreSQL.

## Contributing

Contributions are welcome! Please fork the repository and submit a pull request.

## License

This project is licensed under the Apache License 2.0. See the [LICENSE](http://www.apache.org/licenses/LICENSE-2.0) file for details.
